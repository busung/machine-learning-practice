{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "201711514 박주호_과제9(산데과).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Pcv3GOgxxb2",
        "outputId": "2dff8294-44fb-4ed7-966d-05b16c772f1e"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdriv\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdriv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3hcwfFe2obW"
      },
      "source": [
        "import os\n",
        "data_path = '/content/gdriv/MyDrive/산데과'\n",
        "data_list = os.listdir(data_path)\n",
        "train_dt = pd.read_csv(data_path+'/'+data_list[9]) \n",
        "test_dt = pd.read_csv(data_path+'/'+data_list[10]) "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "hjbUDKjt3IcK",
        "outputId": "9e202449-e7f1-4745-f663-ed789522f9dd"
      },
      "source": [
        "train_dt.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PM</th>\n",
              "      <th>PM2</th>\n",
              "      <th>EG1</th>\n",
              "      <th>EG2</th>\n",
              "      <th>EG3</th>\n",
              "      <th>EG4</th>\n",
              "      <th>EG5</th>\n",
              "      <th>EG6</th>\n",
              "      <th>EG7</th>\n",
              "      <th>EG8</th>\n",
              "      <th>EG9</th>\n",
              "      <th>EG10</th>\n",
              "      <th>EG11</th>\n",
              "      <th>EG12</th>\n",
              "      <th>EG13</th>\n",
              "      <th>EG14</th>\n",
              "      <th>EG15</th>\n",
              "      <th>EG16</th>\n",
              "      <th>EG17</th>\n",
              "      <th>EG18</th>\n",
              "      <th>EG19</th>\n",
              "      <th>EG20</th>\n",
              "      <th>HUM</th>\n",
              "      <th>TMP</th>\n",
              "      <th>WD</th>\n",
              "      <th>WS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1305.26</td>\n",
              "      <td>15069.82</td>\n",
              "      <td>12815.28</td>\n",
              "      <td>2847.84</td>\n",
              "      <td>2847.84</td>\n",
              "      <td>1230.46</td>\n",
              "      <td>14206.22</td>\n",
              "      <td>12080.88</td>\n",
              "      <td>2684.64</td>\n",
              "      <td>2684.64</td>\n",
              "      <td>291.72</td>\n",
              "      <td>3368.04</td>\n",
              "      <td>2864.16</td>\n",
              "      <td>636.48</td>\n",
              "      <td>636.48</td>\n",
              "      <td>583.44</td>\n",
              "      <td>6736.08</td>\n",
              "      <td>5728.32</td>\n",
              "      <td>1272.96</td>\n",
              "      <td>1272.96</td>\n",
              "      <td>91</td>\n",
              "      <td>7.2</td>\n",
              "      <td>0.906308</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>52.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>1021.02</td>\n",
              "      <td>11788.14</td>\n",
              "      <td>10024.56</td>\n",
              "      <td>2227.68</td>\n",
              "      <td>2227.68</td>\n",
              "      <td>729.30</td>\n",
              "      <td>8420.10</td>\n",
              "      <td>7160.40</td>\n",
              "      <td>1591.20</td>\n",
              "      <td>1591.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>437.58</td>\n",
              "      <td>5052.06</td>\n",
              "      <td>4296.24</td>\n",
              "      <td>954.72</td>\n",
              "      <td>954.72</td>\n",
              "      <td>36</td>\n",
              "      <td>14.3</td>\n",
              "      <td>0.438371</td>\n",
              "      <td>1.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>867.68</td>\n",
              "      <td>10017.76</td>\n",
              "      <td>8519.04</td>\n",
              "      <td>1893.12</td>\n",
              "      <td>1893.12</td>\n",
              "      <td>1301.52</td>\n",
              "      <td>15026.64</td>\n",
              "      <td>12778.56</td>\n",
              "      <td>2839.68</td>\n",
              "      <td>2839.68</td>\n",
              "      <td>145.86</td>\n",
              "      <td>1684.02</td>\n",
              "      <td>1432.08</td>\n",
              "      <td>318.24</td>\n",
              "      <td>318.24</td>\n",
              "      <td>291.72</td>\n",
              "      <td>3368.04</td>\n",
              "      <td>2864.16</td>\n",
              "      <td>636.48</td>\n",
              "      <td>636.48</td>\n",
              "      <td>38</td>\n",
              "      <td>6.7</td>\n",
              "      <td>0.992546</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>729.30</td>\n",
              "      <td>8420.10</td>\n",
              "      <td>7160.40</td>\n",
              "      <td>1591.20</td>\n",
              "      <td>1591.20</td>\n",
              "      <td>729.30</td>\n",
              "      <td>8420.10</td>\n",
              "      <td>7160.40</td>\n",
              "      <td>1591.20</td>\n",
              "      <td>1591.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>145.86</td>\n",
              "      <td>1684.02</td>\n",
              "      <td>1432.08</td>\n",
              "      <td>318.24</td>\n",
              "      <td>318.24</td>\n",
              "      <td>23</td>\n",
              "      <td>9.7</td>\n",
              "      <td>0.241922</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>90.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>946.22</td>\n",
              "      <td>10924.54</td>\n",
              "      <td>9290.16</td>\n",
              "      <td>2064.48</td>\n",
              "      <td>2064.48</td>\n",
              "      <td>575.96</td>\n",
              "      <td>6649.72</td>\n",
              "      <td>5654.88</td>\n",
              "      <td>1256.64</td>\n",
              "      <td>1256.64</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>583.44</td>\n",
              "      <td>6736.08</td>\n",
              "      <td>5728.32</td>\n",
              "      <td>1272.96</td>\n",
              "      <td>1272.96</td>\n",
              "      <td>18</td>\n",
              "      <td>5.9</td>\n",
              "      <td>0.484810</td>\n",
              "      <td>1.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     PM   PM2      EG1       EG2       EG3  ...     EG20  HUM   TMP        WD   WS\n",
              "0  27.0  25.0  1305.26  15069.82  12815.28  ...  1272.96   91   7.2  0.906308  0.6\n",
              "1  52.0  17.0  1021.02  11788.14  10024.56  ...   954.72   36  14.3  0.438371  1.3\n",
              "2  22.0  12.0   867.68  10017.76   8519.04  ...   636.48   38   6.7  0.992546  0.5\n",
              "3  23.0  13.0   729.30   8420.10   7160.40  ...   318.24   23   9.7  0.241922  3.0\n",
              "4  90.0  63.0   946.22  10924.54   9290.16  ...  1272.96   18   5.9  0.484810  1.3\n",
              "\n",
              "[5 rows x 26 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "CWWHpyvL3KQ2",
        "outputId": "231da813-8492-4ddf-8e48-e90c0f92c4e6"
      },
      "source": [
        "test_dt.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EG1</th>\n",
              "      <th>EG2</th>\n",
              "      <th>EG3</th>\n",
              "      <th>EG4</th>\n",
              "      <th>EG5</th>\n",
              "      <th>EG6</th>\n",
              "      <th>EG7</th>\n",
              "      <th>EG8</th>\n",
              "      <th>EG9</th>\n",
              "      <th>EG10</th>\n",
              "      <th>EG11</th>\n",
              "      <th>EG12</th>\n",
              "      <th>EG13</th>\n",
              "      <th>EG14</th>\n",
              "      <th>EG15</th>\n",
              "      <th>EG16</th>\n",
              "      <th>EG17</th>\n",
              "      <th>EG18</th>\n",
              "      <th>EG19</th>\n",
              "      <th>EG20</th>\n",
              "      <th>HUM</th>\n",
              "      <th>TMP</th>\n",
              "      <th>WD</th>\n",
              "      <th>WS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>583.44</td>\n",
              "      <td>6736.08</td>\n",
              "      <td>5728.32</td>\n",
              "      <td>1272.96</td>\n",
              "      <td>1272.96</td>\n",
              "      <td>583.44</td>\n",
              "      <td>6736.08</td>\n",
              "      <td>5728.32</td>\n",
              "      <td>1272.96</td>\n",
              "      <td>1272.96</td>\n",
              "      <td>437.58</td>\n",
              "      <td>5052.06</td>\n",
              "      <td>4296.24</td>\n",
              "      <td>954.72</td>\n",
              "      <td>954.72</td>\n",
              "      <td>437.58</td>\n",
              "      <td>5052.06</td>\n",
              "      <td>4296.24</td>\n",
              "      <td>954.72</td>\n",
              "      <td>954.72</td>\n",
              "      <td>68</td>\n",
              "      <td>23.7</td>\n",
              "      <td>0.882948</td>\n",
              "      <td>0.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>875.16</td>\n",
              "      <td>10104.12</td>\n",
              "      <td>8592.48</td>\n",
              "      <td>1909.44</td>\n",
              "      <td>1909.44</td>\n",
              "      <td>1151.92</td>\n",
              "      <td>13299.44</td>\n",
              "      <td>11309.76</td>\n",
              "      <td>2513.28</td>\n",
              "      <td>2513.28</td>\n",
              "      <td>437.58</td>\n",
              "      <td>5052.06</td>\n",
              "      <td>4296.24</td>\n",
              "      <td>954.72</td>\n",
              "      <td>954.72</td>\n",
              "      <td>583.44</td>\n",
              "      <td>6736.08</td>\n",
              "      <td>5728.32</td>\n",
              "      <td>1272.96</td>\n",
              "      <td>1272.96</td>\n",
              "      <td>34</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0.190809</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>729.30</td>\n",
              "      <td>8420.10</td>\n",
              "      <td>7160.40</td>\n",
              "      <td>1591.20</td>\n",
              "      <td>1591.20</td>\n",
              "      <td>1092.08</td>\n",
              "      <td>12608.56</td>\n",
              "      <td>10722.24</td>\n",
              "      <td>2382.72</td>\n",
              "      <td>2382.72</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>437.58</td>\n",
              "      <td>5052.06</td>\n",
              "      <td>4296.24</td>\n",
              "      <td>954.72</td>\n",
              "      <td>954.72</td>\n",
              "      <td>58</td>\n",
              "      <td>11.9</td>\n",
              "      <td>0.754710</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>875.16</td>\n",
              "      <td>10104.12</td>\n",
              "      <td>8592.48</td>\n",
              "      <td>1909.44</td>\n",
              "      <td>1909.44</td>\n",
              "      <td>1600.72</td>\n",
              "      <td>18481.04</td>\n",
              "      <td>15716.16</td>\n",
              "      <td>3492.48</td>\n",
              "      <td>3492.48</td>\n",
              "      <td>437.58</td>\n",
              "      <td>5052.06</td>\n",
              "      <td>4296.24</td>\n",
              "      <td>954.72</td>\n",
              "      <td>954.72</td>\n",
              "      <td>291.72</td>\n",
              "      <td>3368.04</td>\n",
              "      <td>2864.16</td>\n",
              "      <td>636.48</td>\n",
              "      <td>636.48</td>\n",
              "      <td>25</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.275637</td>\n",
              "      <td>1.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1237.94</td>\n",
              "      <td>14292.58</td>\n",
              "      <td>12154.32</td>\n",
              "      <td>2700.96</td>\n",
              "      <td>2700.96</td>\n",
              "      <td>1806.42</td>\n",
              "      <td>20855.94</td>\n",
              "      <td>17735.76</td>\n",
              "      <td>3941.28</td>\n",
              "      <td>3941.28</td>\n",
              "      <td>291.72</td>\n",
              "      <td>3368.04</td>\n",
              "      <td>2864.16</td>\n",
              "      <td>636.48</td>\n",
              "      <td>636.48</td>\n",
              "      <td>1301.52</td>\n",
              "      <td>15026.64</td>\n",
              "      <td>12778.56</td>\n",
              "      <td>2839.68</td>\n",
              "      <td>2839.68</td>\n",
              "      <td>66</td>\n",
              "      <td>13.7</td>\n",
              "      <td>-0.173648</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       EG1       EG2       EG3      EG4  ...  HUM   TMP        WD   WS\n",
              "0   583.44   6736.08   5728.32  1272.96  ...   68  23.7  0.882948  0.7\n",
              "1   875.16  10104.12   8592.48  1909.44  ...   34   3.5  0.190809  3.5\n",
              "2   729.30   8420.10   7160.40  1591.20  ...   58  11.9  0.754710  0.6\n",
              "3   875.16  10104.12   8592.48  1909.44  ...   25   7.0  0.275637  1.9\n",
              "4  1237.94  14292.58  12154.32  2700.96  ...   66  13.7 -0.173648  0.4\n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RH03F0Q3Lzq"
      },
      "source": [
        "dt=pd.concat([train_dt,test_dt])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2v9V7KgC6ae",
        "outputId": "797dd43c-a980-4807-c3ca-0c3a48e9bd42"
      },
      "source": [
        "idx0 = train_dt.loc[train_dt['PM2']<=15,:].index\n",
        "idx1 = train_dt.loc[(train_dt['PM2']>=15)&(train_dt['PM2']<=35),:].index\n",
        "idx2 = train_dt.loc[train_dt['PM2']>=36,:].index\n",
        "\n",
        "train_dt.iloc[idx0,1]=0\n",
        "train_dt.iloc[idx1,1]=1\n",
        "train_dt.iloc[idx2,1]=2\n",
        "\n",
        "train_dt['PM2'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    1670\n",
              "2.0    1033\n",
              "0.0     725\n",
              "Name: PM2, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJO9_oXZDRyw",
        "outputId": "afbf4404-fb08-477b-ce52-14fe4d8a2023"
      },
      "source": [
        "idx0 = train_dt.loc[train_dt['PM2']<=30,:].index\n",
        "idx1 = train_dt.loc[(train_dt['PM2']>=30)&(train_dt['PM2']<=80),:].index\n",
        "idx2 = train_dt.loc[train_dt['PM2']>=81,:].index\n",
        "\n",
        "train_dt.iloc[idx0,0]=0\n",
        "train_dt.iloc[idx1,0]=1\n",
        "train_dt.iloc[idx2,0]=2\n",
        "\n",
        "train_dt['PM2'].value_counts()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    1670\n",
              "2.0    1033\n",
              "0.0     725\n",
              "Name: PM2, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj_n6xjeBUQY"
      },
      "source": [
        "x=dt.iloc[:,2:]\n",
        "pm2=train_dt.iloc[:,1]\n",
        "pm1=train_dt.iloc[:,1]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBjA3-_SBReY",
        "outputId": "e516994a-81bd-41e3-d3dc-1fabfc1347f4"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "min_max_scaler = MinMaxScaler()\n",
        "min_max_scaler.fit(x)\n",
        "x = min_max_scaler.transform(x)\n",
        "x"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.64510166, 0.64510166, 0.64510166, ..., 0.4469914 , 0.95315033,\n",
              "        0.10909091],\n",
              "       [0.50462107, 0.50462107, 0.50462107, ..., 0.6504298 , 0.71916416,\n",
              "        0.23636364],\n",
              "       [0.42883549, 0.42883549, 0.42883549, ..., 0.43266476, 0.99627282,\n",
              "        0.09090909],\n",
              "       ...,\n",
              "       [0.50462107, 0.50462107, 0.50462107, ..., 0.48137536, 0.75750058,\n",
              "        0.23636364],\n",
              "       [0.2883549 , 0.2883549 , 0.2883549 , ..., 0.32091691, 0.68727949,\n",
              "        0.10909091],\n",
              "       [0.75231054, 0.75231054, 0.75231054, ..., 0.58739255, 0.97275723,\n",
              "        0.16363636]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMA7Tnp1BqFN"
      },
      "source": [
        "train_x=pd.DataFrame(x).iloc[:3428,:]\n",
        "test_x = pd.DataFrame(x).iloc[3429:]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlBqtXEbCPNn"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "pm2_trainx,pm2_testx,pm2_trainy,pm2_testy = train_test_split(train_x,pm2,test_size=0.3,random_state=15)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OryUkODCvCx"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "pm1_trainx,pm1_testx,pm1_trainy,pm1_testy = train_test_split(train_x,pm1,test_size=0.3,random_state=15)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqM9RIWWEGfC"
      },
      "source": [
        "pm2_trainx=pm2_trainx.to_numpy()\n",
        "pm2_testx=pm2_testx.to_numpy()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG22bBz5EP2g"
      },
      "source": [
        "pm1_trainx=pm1_trainx.to_numpy()\n",
        "pm1_testx=pm1_testx.to_numpy()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O21BvkrsCTII",
        "outputId": "126aa15c-c070-403d-cbf6-9be61e661247"
      },
      "source": [
        "pm2_testy"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2690    2.0\n",
              "2978    1.0\n",
              "2984    0.0\n",
              "670     2.0\n",
              "2845    1.0\n",
              "       ... \n",
              "1032    2.0\n",
              "3140    0.0\n",
              "1706    2.0\n",
              "2049    1.0\n",
              "659     1.0\n",
              "Name: PM2, Length: 1029, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctGrwwxXDyVN"
      },
      "source": [
        "#pm2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOybwTtzDx34"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP2S5V05D1oZ"
      },
      "source": [
        "class mlp(nn.Module):\n",
        "\n",
        "  def __init__(self,input_size):\n",
        "    super(mlp,self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.fc1 = nn.Linear(self.input_size,18)\n",
        "    self.bn1 = torch.nn.BatchNorm1d(18)\n",
        "    self.fc2 = nn.Linear(18,9)\n",
        "    self.bn2 = torch.nn.BatchNorm1d(9)\n",
        "    self.fc4 = nn.Linear(9,3)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.dropout = torch.nn.Dropout()\n",
        "    self.sf=torch.nn.Softmax()\n",
        "\n",
        "  def forward(self,x):#Linear -> BatchNorm -> function -> Dropout 순서로 구성\n",
        "    output = self.fc1(x)\n",
        "    output = self.bn1(output)\n",
        "    output = self.relu(output)\n",
        "    output = self.dropout(output)\n",
        "    output = self.fc2(output)\n",
        "    output = self.bn2(output)\n",
        "    output = self.relu(output)\n",
        "    output = self.dropout(output)\n",
        "    output = self.fc4(output)\n",
        "    output = self.sf(output)\n",
        "    return output"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK9IE256D9Jx"
      },
      "source": [
        "pm2_trainx = torch.FloatTensor(pm2_trainx)\n",
        "pm2_testx = torch.FloatTensor(pm2_testx)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0_hLgx8EeeD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "985cd6fc-c52b-4d14-ae46-218844f9795c"
      },
      "source": [
        "pm2_trainy = torch.FloatTensor(pm2_trainy.values)\n",
        "pm2_trainy = pm2_trainy.long()# output이 정수여야 하기에 long으로 casting해 줌\n",
        "\n",
        "pm2_testy = torch.FloatTensor(pm2_testy.values)\n",
        "pm2_testy = pm2_testy.long()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-32b400476ee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpm2_trainy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpm2_trainy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpm2_trainy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm2_trainy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# output이 정수여야 하기에 long으로 casting해 줌\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpm2_testy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpm2_testy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpm2_testy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm2_testy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: new(): data must be a sequence (got builtin_function_or_method)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_sVVK7VEhZW"
      },
      "source": [
        "input_size = pm2_trainx.size()[1]\n",
        "model = mlp(input_size)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyBT6KXxEik2"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.0001)#lr = learning rate(gradient descent에서 그 lambda값)\n",
        "loss = nn.CrossEntropyLoss()#다중분류 문제이기 때문에"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQIn0StVEjmq",
        "outputId": "54bb7d26-1ce0-440d-ea3d-cf7e1a0eb4d3"
      },
      "source": [
        "for e in range(15000):#epoch 즉 학습 횟수를 5000으로 지정하겠다\n",
        "  optimizer.zero_grad()#optimizer를 zero로 초기화 시켜줌\n",
        "  out = model(pm2_trainx)\n",
        "  L = loss(out,pm2_trainy)\n",
        "  L.backward()#역전파 과정\n",
        "  optimizer.step()#파라미터를 최적화 시키는 과정\n",
        "  if (e+1) % 500 == 0:#epoch이 500일 때 마다 모델을 평가해주는 부분\n",
        "    with torch.no_grad():#테스트 할 때는 변형하지 않기 위하여 추가해주는 함수\n",
        "      model.eval\n",
        "      pred = model(pm2_testx)\n",
        "      acc = sum(torch.max(pred,1)[1] == pm2_testy).numpy()/len(pm2_testy)#torch.max[0]=확률을,[1]=indicator를 반환\n",
        "    print('[Epoch: {}/{}] [Loss: {:2f}] [Test Accuracy: {:2f}]'.format(e+1,15000,L.item(),acc*100))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 500/5000] [Loss: 1.075810] [Test Accuracy: 46.550049]\n",
            "[Epoch: 1000/5000] [Loss: 1.048351] [Test Accuracy: 50.242954]\n",
            "[Epoch: 1500/5000] [Loss: 1.031502] [Test Accuracy: 52.478134]\n",
            "[Epoch: 2000/5000] [Loss: 1.021776] [Test Accuracy: 53.741497]\n",
            "[Epoch: 2500/5000] [Loss: 1.008969] [Test Accuracy: 53.158406]\n",
            "[Epoch: 3000/5000] [Loss: 1.011169] [Test Accuracy: 50.534500]\n",
            "[Epoch: 3500/5000] [Loss: 1.002991] [Test Accuracy: 54.324587]\n",
            "[Epoch: 4000/5000] [Loss: 0.996988] [Test Accuracy: 54.324587]\n",
            "[Epoch: 4500/5000] [Loss: 0.989703] [Test Accuracy: 55.102041]\n",
            "[Epoch: 5000/5000] [Loss: 0.992004] [Test Accuracy: 54.324587]\n",
            "[Epoch: 5500/5000] [Loss: 0.990329] [Test Accuracy: 54.033042]\n",
            "[Epoch: 6000/5000] [Loss: 0.985935] [Test Accuracy: 54.324587]\n",
            "[Epoch: 6500/5000] [Loss: 0.980741] [Test Accuracy: 54.518950]\n",
            "[Epoch: 7000/5000] [Loss: 0.976875] [Test Accuracy: 56.365403]\n",
            "[Epoch: 7500/5000] [Loss: 0.986214] [Test Accuracy: 53.838678]\n",
            "[Epoch: 8000/5000] [Loss: 0.980143] [Test Accuracy: 53.741497]\n",
            "[Epoch: 8500/5000] [Loss: 0.977251] [Test Accuracy: 56.171040]\n",
            "[Epoch: 9000/5000] [Loss: 0.974085] [Test Accuracy: 57.240039]\n",
            "[Epoch: 9500/5000] [Loss: 0.974835] [Test Accuracy: 57.142857]\n",
            "[Epoch: 10000/5000] [Loss: 0.978532] [Test Accuracy: 56.559767]\n",
            "[Epoch: 10500/5000] [Loss: 0.977222] [Test Accuracy: 55.976676]\n",
            "[Epoch: 11000/5000] [Loss: 0.969857] [Test Accuracy: 56.851312]\n",
            "[Epoch: 11500/5000] [Loss: 0.970345] [Test Accuracy: 55.587949]\n",
            "[Epoch: 12000/5000] [Loss: 0.974105] [Test Accuracy: 56.559767]\n",
            "[Epoch: 12500/5000] [Loss: 0.965969] [Test Accuracy: 57.628766]\n",
            "[Epoch: 13000/5000] [Loss: 0.972091] [Test Accuracy: 57.337221]\n",
            "[Epoch: 13500/5000] [Loss: 0.971680] [Test Accuracy: 56.073858]\n",
            "[Epoch: 14000/5000] [Loss: 0.968118] [Test Accuracy: 54.324587]\n",
            "[Epoch: 14500/5000] [Loss: 0.966524] [Test Accuracy: 58.503401]\n",
            "[Epoch: 15000/5000] [Loss: 0.966729] [Test Accuracy: 55.685131]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RsaM-XKFRxk"
      },
      "source": [
        "test_x_=test_x.to_numpy()\n",
        "test_x_=torch.FloatTensor(test_x_)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqICgjO5FJii",
        "outputId": "ce8bced6-fad2-4e7e-8d83-bfd329029f39"
      },
      "source": [
        "pred = model(test_x_)\n",
        "result=pd.DataFrame(torch.max(pred,1)[1])\n",
        "result.to_csv(\"201711514 박주호 pm2.csv\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_EZTxOWFDgp"
      },
      "source": [
        "#pm1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g3-psiHGHfC"
      },
      "source": [
        "pm1_trainx = torch.FloatTensor(pm1_trainx)\n",
        "pm1_testx = torch.FloatTensor(pm1_testx)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf4rLl0CGIvd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "48ce77c7-bcc7-4b30-e2ac-a68ab1f1453d"
      },
      "source": [
        "pm1_trainy = torch.FloatTensor(pm1_trainy.values)\n",
        "pm1_trainy = pm1_trainy.long()# output이 정수여야 하기에 long으로 casting해 줌\n",
        "\n",
        "pm1_testy = torch.FloatTensor(pm1_testy.values)\n",
        "pm1_testy = pm1_testy.long()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-6b0a84c2c1b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpm1_trainy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpm1_trainy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpm1_trainy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm1_trainy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# output이 정수여야 하기에 long으로 casting해 줌\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpm1_testy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpm1_testy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpm1_testy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm1_testy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: new(): data must be a sequence (got builtin_function_or_method)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKfr-iiVF2ts"
      },
      "source": [
        "model_2 = mlp(input_size)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eug9eqALF831"
      },
      "source": [
        "optimizer = optim.Adam(model_2.parameters(),lr=0.0001)#lr = learning rate(gradient descent에서 그 lambda값)\n",
        "loss = nn.CrossEntropyLoss()#다중분류 문제이기 때문에"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSpNUBatF_GW",
        "outputId": "94682766-6871-49ef-ced0-0026c06bb035"
      },
      "source": [
        "for e in range(15000):#epoch 즉 학습 횟수를 5000으로 지정하겠다\n",
        "  optimizer.zero_grad()#optimizer를 zero로 초기화 시켜줌\n",
        "  out = model(pm1_trainx)\n",
        "  L = loss(out,pm1_trainy)\n",
        "  L.backward()#역전파 과정\n",
        "  optimizer.step()#파라미터를 최적화 시키는 과정\n",
        "  if (e+1) % 500 == 0:#epoch이 500일 때 마다 모델을 평가해주는 부분\n",
        "    with torch.no_grad():#테스트 할 때는 변형하지 않기 위하여 추가해주는 함수\n",
        "      model.eval\n",
        "      pred = model(pm1_testx)\n",
        "      acc = sum(torch.max(pred,1)[1] == pm1_testy).numpy()/len(pm1_testy)#torch.max[0]=확률을,[1]=indicator를 반환\n",
        "    print('[Epoch: {}/{}] [Loss: {:2f}] [Test Accuracy: {:2f}]'.format(e+1,15000,L.item(),acc*100))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 500/15000] [Loss: 0.965841] [Test Accuracy: 54.616132]\n",
            "[Epoch: 1000/15000] [Loss: 0.965168] [Test Accuracy: 57.337221]\n",
            "[Epoch: 1500/15000] [Loss: 0.964866] [Test Accuracy: 57.823129]\n",
            "[Epoch: 2000/15000] [Loss: 0.965851] [Test Accuracy: 54.907677]\n",
            "[Epoch: 2500/15000] [Loss: 0.970392] [Test Accuracy: 55.976676]\n",
            "[Epoch: 3000/15000] [Loss: 0.969179] [Test Accuracy: 56.073858]\n",
            "[Epoch: 3500/15000] [Loss: 0.961632] [Test Accuracy: 56.559767]\n",
            "[Epoch: 4000/15000] [Loss: 0.965769] [Test Accuracy: 56.851312]\n",
            "[Epoch: 4500/15000] [Loss: 0.975557] [Test Accuracy: 56.268222]\n",
            "[Epoch: 5000/15000] [Loss: 0.971241] [Test Accuracy: 56.268222]\n",
            "[Epoch: 5500/15000] [Loss: 0.965815] [Test Accuracy: 56.365403]\n",
            "[Epoch: 6000/15000] [Loss: 0.966613] [Test Accuracy: 56.948494]\n",
            "[Epoch: 6500/15000] [Loss: 0.971380] [Test Accuracy: 57.434402]\n",
            "[Epoch: 7000/15000] [Loss: 0.966402] [Test Accuracy: 57.920311]\n",
            "[Epoch: 7500/15000] [Loss: 0.969388] [Test Accuracy: 56.073858]\n",
            "[Epoch: 8000/15000] [Loss: 0.978073] [Test Accuracy: 56.656948]\n",
            "[Epoch: 8500/15000] [Loss: 0.968611] [Test Accuracy: 57.337221]\n",
            "[Epoch: 9000/15000] [Loss: 0.964374] [Test Accuracy: 56.365403]\n",
            "[Epoch: 9500/15000] [Loss: 0.962868] [Test Accuracy: 56.559767]\n",
            "[Epoch: 10000/15000] [Loss: 0.962418] [Test Accuracy: 56.656948]\n",
            "[Epoch: 10500/15000] [Loss: 0.977393] [Test Accuracy: 57.045675]\n",
            "[Epoch: 11000/15000] [Loss: 0.970053] [Test Accuracy: 57.531584]\n",
            "[Epoch: 11500/15000] [Loss: 0.966676] [Test Accuracy: 56.073858]\n",
            "[Epoch: 12000/15000] [Loss: 0.962760] [Test Accuracy: 58.211856]\n",
            "[Epoch: 12500/15000] [Loss: 0.973735] [Test Accuracy: 57.045675]\n",
            "[Epoch: 13000/15000] [Loss: 0.962880] [Test Accuracy: 57.045675]\n",
            "[Epoch: 13500/15000] [Loss: 0.967753] [Test Accuracy: 57.823129]\n",
            "[Epoch: 14000/15000] [Loss: 0.968188] [Test Accuracy: 55.587949]\n",
            "[Epoch: 14500/15000] [Loss: 0.962200] [Test Accuracy: 56.754130]\n",
            "[Epoch: 15000/15000] [Loss: 0.961482] [Test Accuracy: 57.045675]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f3XmrsbGVgu",
        "outputId": "71dfee31-c678-471f-9649-8120ec5b8ab5"
      },
      "source": [
        "pred = model(test_x_)\n",
        "result_pm1=pd.DataFrame(torch.max(pred,1)[1])\n",
        "result_pm1.to_csv(\"201711514 박주호 pm1.csv\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    }
  ]
}